#  
#

from math import pi
from itertools import chain
import random
import pdb #debugger


from topo import *
from topo.base.patterngenerator import *
from topo.sheets.generatorsheet import *
from topo.plotting.plotfilesaver import ImageSaver
import topo.base.cfsheet
from topo.base.parameter import *
from topo.base.utils import *
from topo.base.learningrules import *
import weave
import Numeric
import fixedpoint
from fixedpoint import FixedPoint
from topo.patterns.random import UniformRandomGenerator
from topo.patterns.basic import GaussianGenerator
from topo.projections.kernelprojection import KernelProjection


from topo.base.utils import msum
"""
TODO:

x boundingregion.translate()
- composing kernel factories (operator overloading?)

x LISSOM learning rule (just mdot)
x xfer funcs  (piecewise linear sigmoid)

- port parameterization (in cfsheet? in sheet? in EventProcessor?)
"""



class LISSOM(topo.base.cfsheet.CFSheet):

    transfer_fn = Parameter(default=topo.base.utils.PLTF)
    learning_fn = Parameter(default=hebbian_c)

    # modify weights after each activation?
    continuous_learning = Parameter(default=False)

    presleep_count = 0

    def input_event(self,src,src_port,dest_port,data):
        # On a new afferent input, clear the activity
        if src.name == 'Retina':
            self.activity *= 0.0
            for name in self.projections:
                for proj in self.projections[name]:
                    proj.temp_activity *= 0.0

        super(LISSOM,self).input_event(src,src_port,dest_port,data)

    def pre_sleep(self):
        """
        Pass the accumulated stimulation through self.transfer_fn and
        send it out on the default output port.
        """

        iteration_done = False
        self.presleep_count += 1

	if self.presleep_count == tsettle+1: #end of an iteration
            iteration_done = True

        if self.new_input:
            self.new_input = False
            self.temp_activity *= 0.0
            for name in self.projections:
                for proj in self.projections[name]:
                    self.temp_activity += proj.temp_activity
            self.activity = self.transfer_fn(self.temp_activity,delta,beta)
            # don't send output when an iteration has ended
            if not iteration_done: 
                self.send_output(data=self.activity)

        if self._learning:
            if self.continuous_learning:
                self.learn()
            else:
                if iteration_done:
                    self.presleep_count = 0
                    self.learn()
                    

    def learn(self):
        rows,cols = self.activity.shape
        for proj in chain(*self.projections.values()):
            if proj.input_buffer:
                alpha = proj.learning_rate
                if proj.src == self: #lateral connection
                    inp = self.activity
                else:
                    inp = proj.input_buffer

                if self.learning_fn.func_name == "hebbian_c":
                    # hebbian_c is an function for the whole sheet
                    cfs = proj.cfs
                    len, len2 = inp.shape
		    if proj.normalize:
                        # Hebbian learning with divisive normalization per 
                        # projection. It is much faster to do both of them
                        # in one function.
			if proj.normalize_fn.func_name == "divisive_normalization":
                            hebbian_div_norm_c(inp, self.activity, rows, cols, len, cfs, alpha)
                        else:
                            hebbian_c(inp, self.activity, rows, cols, len, cfs, alpha)
			    norm = proj.normalize
                            for r in range(rows):
                                for c in range(cols):
                                    proj.normalize_fn(cfs[r][c].weights)
                    else:
                        hebbian_c(inp, self.activity, rows, cols, len, cfs, alpha)
                else:
                    # apply learning rule and normalization to each unit
                    norm = proj.normalize
                    for r in range(rows):
                        for c in range(cols):
                            cf = proj.cf(r,c)
                            self.learning_fn(cf.get_input_matrix(inp), self.activity[r,c], cf.weights, alpha)
                            if norm:
                                proj.normalize_fn(cf.weights)


    def lateral_projections(self):
        return [p for p in chain(*self.projections.values()) if p.src is self]
    def afferent_projections(self):
        return [p for p in chain(*self.projections.values()) if p.src is not self]

    def reduce_cfsize(self, name, new_wt_bounds):
        for proj in chain(*self.projections.values()):
            if proj.name == name:
                proj.reduce_cfsize(new_wt_bounds)
                return
        self.warning("Can't find ", name)

    def change_learning_rate(self, name, new_alpha):
        for proj in chain(*self.projections.values()):
            if proj.name == name:
                proj.learning_rate = new_alpha
                return
        self.warning("Can't find ", name)


    # print the weights of a unit
    def printwts(self,x,y):
        for proj in chain(*self.projections.values()):
            print proj.name, x, y
            print transpose(proj.cfs[x][y].weights)


###########################################
# Set parameters



# Temporary variables designed to match the C++ version of lissom
BaseN=72.0
BaseRN=24.0
area_scale=1.0
num_eyes=1
rf_radius=BaseRN/4.0+0.5
inh_rad=max(2.0,(BaseN/4.0-1.5)+0.5)
exc_rad=max(2.0,(BaseN/10.0-0.5)+0.5)
default_afferent_size_scale=BaseN/BaseRN
gammaexc=0.9
gammainh=0.9
delta=0.1
beta=0.65
#min_exc_rad=max(1,BaseN/44+0.5)

rf_radius_scale=6.0/rf_radius
#retina_area_scale=(BaseRN*area_scale/24.0*rf_radius_scale)*(BaseRN*area_scale/24.0*rf_radius_scale)
#inputs_pereye=max(retina_area_scale,1)
#uncorrelation=0.0 
xsigma=6.0*rf_radius_scale
ysigma=1.5*rf_radius_scale

#scale_input=1.0
retina_edge_buffer=0*(rf_radius+(BaseRN*area_scale/2))
RN=BaseRN*area_scale+2*retina_edge_buffer

acs=6.5*6.5/rf_radius/rf_radius
ecs=19.5*19.5/exc_rad/exc_rad
ics=47.5*47.5/inh_rad/inh_rad

alpha_input=0.007*acs
alpha_exc=0.002*ecs
alpha_inh=0.00025*ics

# Number of decimal places for simulator time
fixedpoint.DEFAULT_PRECISION=3

tsettle=8

print "Setting parameters..."


# input generation params
# jbednar050411: Doesn't seem to work with anything but 900 for both densities (!)
GeneratorSheet.density = RN*RN
GeneratorSheet.period = 1

GaussianGenerator.x = Dynamic(lambda : random.uniform(-0.5,0.5))
GaussianGenerator.y = Dynamic(lambda : random.uniform(-0.5,0.5))

GaussianGenerator.theta = Dynamic(lambda :random.uniform(-pi,pi))
GaussianGenerator.width = xsigma/RN
GaussianGenerator.height = ysigma/RN
GaussianGenerator.bounds = BoundingBox(points=((-0.5,-0.5),(0.5,0.5)))


# lissom parameters
LISSOM.density = BaseN*BaseN


# image saver parameters
ImageSaver.file_format='png'
ImageSaver.time_format='%0.4d'

# Connection parameters
afferent_weight_bounds   = BoundingBox(points=((-rf_radius/RN,-rf_radius/RN),(rf_radius/RN,rf_radius/RN)))
excitatory_weight_bounds = BoundingBox(points=((-exc_rad/BaseN,-exc_rad/BaseN),(exc_rad/BaseN,exc_rad/BaseN)))
inhibitory_weight_bounds = BoundingBox(points=((-inh_rad/BaseN,-inh_rad/BaseN),(inh_rad/BaseN,inh_rad/BaseN)))
###########################################
# build simulation


print "Creating simulation objects..."

s = topo.base.simulator.Simulator()

retina = GeneratorSheet(input_generator=GaussianGenerator(),name='Retina')
V1 = LISSOM(name='V1')
save  = ImageSaver(name='LISSOM')

s.connect(retina,V1, delay = FixedPoint("0.05"),
          projection_type=KernelProjection,
          projection_params = dict(strength = 1.0, name='Afferent0',
                                   weights_bounds = afferent_weight_bounds, normalize = True, activation_fn=compute_response_mdot_c,learning_rate=alpha_input))


s.connect(V1,V1, delay = FixedPoint("0.05"), dest_port="exc",
          projection_type=KernelProjection,
          projection_params = dict(strength = gammaexc, name='LateralExcitatory',
                                   weights_bounds = excitatory_weight_bounds, normalize = True, activation_fn=compute_response_mdot_c,learning_rate=alpha_exc))

s.connect(V1,V1, delay = FixedPoint("0.05"), dest_port="inh",
          projection_type=KernelProjection,
          projection_params = dict(strength = -gammainh, name='LateralInhibitory',
                                   weights_bounds = inhibitory_weight_bounds, normalize = True, activation_fn=compute_response_mdot_c,learning_rate=alpha_inh))
        

#################################################
# Schedule the changes of simulation parameters

s.schedule_action(200, V1.reduce_cfsize, "LateralExcitatory", BoundingBox(points=((-exc_rad*0.6/BaseN,-exc_rad*0.6/BaseN),(exc_rad*0.6/BaseN,exc_rad*0.6/BaseN))))

s.schedule_action(500, V1.reduce_cfsize, "LateralExcitatory", BoundingBox(points=((-exc_rad*0.42/BaseN,-exc_rad*0.42/BaseN),(exc_rad*0.42/BaseN,exc_rad*0.42/BaseN))))

s.schedule_action(1000, V1.reduce_cfsize, "LateralExcitatory", BoundingBox(points=((-exc_rad*0.336/BaseN,-exc_rad*0.336/BaseN),(exc_rad*0.336/BaseN,exc_rad*0.336/BaseN))))

s.schedule_action(2000, V1.reduce_cfsize, "LateralExcitatory", BoundingBox(points=((-2.0/BaseN,-2.0/BaseN),(2.0/BaseN,2.0/BaseN))))

s.schedule_action(500, V1.change_learning_rate, "Afferent0", 0.005*acs)
s.schedule_action(2000, V1.change_learning_rate, "Afferent0", 0.004*acs)
s.schedule_action(4000, V1.change_learning_rate, "Afferent0", 0.003*acs)
s.schedule_action(20000, V1.change_learning_rate, "Afferent0", 0.0015*acs)

s.schedule_action(500, V1.change_learning_rate, "LateralExcitatory", 0.001*ecs)


#function that set delta and beta for the threshold function
def set_deltabeta(x,y): 
    globals()["delta"]=x
    globals()["beta"]=y


s.schedule_action(200, set_deltabeta, 0.11, 0.66)
s.schedule_action(500, set_deltabeta, 0.12, 0.67)
s.schedule_action(1000, set_deltabeta, 0.15, 0.68)
s.schedule_action(2000, set_deltabeta, 0.18, 0.70)
s.schedule_action(3000, set_deltabeta, 0.20, 0.73)
s.schedule_action(4000, set_deltabeta, 0.20, 0.76)
s.schedule_action(5000, set_deltabeta, 0.21, 0.79)
s.schedule_action(6500, set_deltabeta, 0.22, 0.82)
s.schedule_action(8000, set_deltabeta, 0.23, 0.85)
s.schedule_action(20000, set_deltabeta, 0.24, 0.88)

#s.schedule_action(20009.99, V1.printwts, 36, 36)

#cfsheet.CF.print_level = base.VERBOSE

# Uncomment the connections to the image saver, to save all the activity
# images to disk.
#s.connect(retina,save,dest_port='retina',delay=2)
#s.connect(V1,save,dest_port='V1',delay=1)

# can either pass a number or FixedPoint object to s.run()
#s.run(10)
#s.run(FixedPoint(10))

#V1.projections['Retina'][0].plot_cfs()

#import hotshot,hotshot.stats
#prof = hotshot.Profile("abc")
#prof.runctx('s.run(50)',globals(),locals())
#prof.close()
#
#p = hotshot.stats.load("abc")
#p.strip_dirs().sort_stats('time').print_stats()



#import profile,pstats
#profile.runctx('s.run(2000)',globals(),locals(),"abc")
#p = pstats.Stats('abc')
#p.strip_dirs().sort_stats('cumulative').print_stats()
