"""
LISSOM-based orientation map (without ON/OFF channels) 
with Homeostatic Intrinsic Plasticity Mechanism

"""
__version__='$Revision$'

import RandomArray
import fixedpoint
import copy

from math import pi, sqrt
from fixedpoint import FixedPoint
from Numeric import sum,ones,exp
import Numeric

import topo.patterns.basic
import topo.patterns.random

from topo.sheets.lissom import LISSOM
from topo.sheets.generatorsheet import GeneratorSheet
from topo.projections.basic import CFProjection
from topo.responsefns.optimized import CFPRF_DotProduct_opt
from topo.base.parameterclasses import DynamicNumber, Number
from topo.base.cf import CFSheet
from topo.base.boundingregion import BoundingBox
from topo.learningfns.optimized import CFPLF_Hebbian_opt
from topo.outputfns.optimized import CFPOF_DivisiveNormalizeL1_opt
from topo.outputfns.basic import PiecewiseLinear
from topo.misc.numbergenerators import UniformRandom
from topo.base.functionfamilies import OutputFn
from topo.base.arrayutils import clip_in_place
from topo.commands.pylabplots import *
from matplotlib.pylab import *

topo.sim.name = "homeo_intrinsic_lissom_or"

input_pattern = topo.patterns.basic.Gaussian(
          scale=1.0, size=2*0.0468, aspect_ratio=4.0,
          x=DynamicNumber(UniformRandom(lbound=-0.5,ubound=0.5,seed=12)),
          y=DynamicNumber(UniformRandom(lbound=-0.5,ubound=0.5,seed=34)),
          orientation=DynamicNumber(UniformRandom(lbound=-pi,ubound=pi,seed=56)))

                                 
# Specify weight initialization, response function, and learning function
RandomArray.seed(500,500)
CFProjection.weights_generator=topo.patterns.random.UniformRandom()
CFProjection.weights_shape=topo.patterns.basic.Disk(smoothing=0.0)
CFProjection.response_fn=CFPRF_DotProduct_opt()
CFProjection.learning_fn=CFPLF_Hebbian_opt()
CFProjection.weights_output_fn=CFPOF_DivisiveNormalizeL1_opt()


class HomeostaticMaxEnt(OutputFn):
    """
    Implementation of homeostatic intrinsic excitability from Triesch, ICANN 2005, LNCS 3696 pp. 65-70.
    """
    lstep = Number(default=8,doc="")
    a_init = Number(default=13,doc="Multiplicative parameter controlling the exponential.")
    b_init = Number(default=-4,doc="Additive parameter controlling the exponential.")
    eta = Number(default=0.0002,doc="Learning rate for homeostatic plasticity.")
    mu = Number(default=0.01,doc="Average firing rate target.")


    def __init__(self,**params):
        super(HomeostaticMaxEnt,self).__init__(**params)

	self.first_call = True

	# DEBUG only: Not required for the algorithm, but useful for debugging
	"""
	self.y_avg_count = 0 
	self.n_step = 0
	self.beta = 0.0003
	self.ncall = 0
	"""

    def __call__(self,x):
	
	if self.first_call:
	    self.first_call = False
	    self.a = Numeric.ones(x.shape, x.typecode()) * self.a_init
	    self.b = Numeric.ones(x.shape, x.typecode()) * self.b_init

	    # DEBUG only (for computing average activity of each neuron over time) 
	    """
	    self.y_avg = zeros(x.shape,x.typecode())
	    self.y_hist = zeros([5,30000],Float)  # average firing rate
	    self.y_rate = zeros([5,30000],Float)  # firing rate over time
	    self.x_rate = zeros([5,30000],Float)  # input to postsynaptic neurons, before thresholding
	    self.ab_hist = zeros([2,5,30000],Float) # History of a and b
	    """

	# Apply sigmoid function to x, resulting in what Triesch calls y
        x_orig = copy.copy(x)
        x *= 0.0
	x += 1.0 / (1.0 + exp(-(self.a*x_orig + self.b)))

	# DEBUG only (computing average activity of each neuron over time) 
	"""
	self.n_step += 1	
	if self.n_step == self.lstep:
	    self.n_step = 0
	
	    self.ncall += 1
	    if self.ncall <= 500:
	        self.y_avg = ((self.y_avg*self.ncall) + x) / (self.ncall+1)
	    else:
	        self.y_avg = self.beta*x + (1.0-self.beta)*self.y_avg

	    # Average firing rate
	    self.y_hist[0][self.ncall] = self.y_avg[0][0]
	    self.y_hist[1][self.ncall] = self.y_avg[11][11]
	    self.y_hist[2][self.ncall] = self.y_avg[23][23]
	    self.y_hist[3][self.ncall] = self.y_avg[35][35]
	    self.y_hist[4][self.ncall] = self.y_avg[47][47]

	    # firing rate over time
	    self.y_rate[0][self.ncall] = x[0][0]
	    self.y_rate[1][self.ncall] = x[11][11]
	    self.y_rate[2][self.ncall] = x[23][23]
	    self.y_rate[3][self.ncall] = x[35][35]
	    self.y_rate[4][self.ncall] = x[47][47]

	    # input to postsynaptic neurons, before thresholding
	    self.x_rate[0][self.ncall] = x_orig[0][0]
	    self.x_rate[1][self.ncall] = x_orig[11][11]
	    self.x_rate[2][self.ncall] = x_orig[23][23]
	    self.x_rate[3][self.ncall] = x_orig[35][35]
	    self.x_rate[4][self.ncall] = x_orig[47][47]

	    # a & b
	    self.ab_hist[0][0][self.ncall] = self.a[0][0]
	    self.ab_hist[0][1][self.ncall] = self.a[11][11]
	    self.ab_hist[0][2][self.ncall] = self.a[23][23]
	    self.ab_hist[0][3][self.ncall] = self.a[35][35]
	    self.ab_hist[0][4][self.ncall] = self.a[47][47]

	    self.ab_hist[1][0][self.ncall] = self.b[0][0]
	    self.ab_hist[1][1][self.ncall] = self.b[11][11]
	    self.ab_hist[1][2][self.ncall] = self.b[23][23]
	    self.ab_hist[1][3][self.ncall] = self.b[35][35]
	    self.ab_hist[1][4][self.ncall] = self.b[47][47]
   
	    print self.y_avg[23][23], ' - ', self.y_avg[11][11], ' - ', self.y_avg[0][0], ' - ', self.y_avg[47][47]
	    print self.a[23][23], ',', self.b[23][23] , ' : ', self.a[11][11], ',', self.b[11][11]
	"""

	# Update a and b
	self.a += self.eta * (1.0/self.a + x_orig - (2.0 + 1.0/self.mu)*x_orig*x + x_orig*x*x/self.mu)
	self.b += self.eta * (1.0 - (2.0 + 1.0/self.mu)*x + x*x/self.mu)



# DEBUG Only: OutputFn for computing average activity of each neuron
class TestFn(OutputFn):
    """ 
    Piecewise-linear output function with lower and upper thresholds
    as constructor parameters.
    """
    lower_bound = Number(default=0.0,softbounds=(0.0,1.0))
    upper_bound = Number(default=1.0,softbounds=(0.0,1.0))
    learn_step = Number(default=8,doc="Number of step before learning take place")

    def __init__(self,**params):
        super(TestFn,self).__init__(**params)

	self.first_call = True
	self.n_step = 0
	self.beta = 0.0003
	self.ncall = 0

    def __call__(self,x):

        x_orig = copy.copy(x)
        fact = 1.0/(self.upper_bound-self.lower_bound)
        x -= self.lower_bound
        x *= fact
        clip_in_place(x,0.0,1.0)

	if self.first_call:
	    self.first_call = False
	    self.y_avg = zeros(x.shape,x.typecode())
	    self.y_hist = zeros([5,30000],Float)  # average firing rate
	    self.y_rate = zeros([5,30000],Float)  # firing rate over time
	    self.y_rate = zeros([5,30000],Float)  # firing rate over time
	    self.x_rate = zeros([5,30000],Float)  # input to postsynaptic neurons, before thresholding

	self.n_step += 1
	if self.n_step == self.learn_step:

	    self.n_step = 0
	    	
	    self.ncall += 1
	    if self.ncall <= 500:
	    	self.y_avg = ((self.y_avg*self.ncall) + x) / (self.ncall+1)
	    else:	    
	    	self.y_avg = self.beta*x + (1.0-self.beta)*self.y_avg

	    # Average firing rate
	    self.y_hist[0][self.ncall] = self.y_avg[0][0]
	    self.y_hist[1][self.ncall] = self.y_avg[11][11]
	    self.y_hist[2][self.ncall] = self.y_avg[23][23]
	    self.y_hist[3][self.ncall] = self.y_avg[35][35]
	    self.y_hist[4][self.ncall] = self.y_avg[47][47]

	    # firing rate over time
	    self.y_rate[0][self.ncall] = x[0][0]
	    self.y_rate[1][self.ncall] = x[11][11]
	    self.y_rate[2][self.ncall] = x[23][23]
	    self.y_rate[3][self.ncall] = x[35][35]
	    self.y_rate[4][self.ncall] = x[47][47]

	    # input to postsynaptic neurons, before thresholding
	    self.x_rate[0][self.ncall] = x_orig[0][0]
	    self.x_rate[1][self.ncall] = x_orig[11][11]
	    self.x_rate[2][self.ncall] = x_orig[23][23]
	    self.x_rate[3][self.ncall] = x_orig[35][35]
	    self.x_rate[4][self.ncall] = x_orig[47][47]

	    print self.y_avg[23][23], ' ', self.y_avg[11][11], ' ', self.y_avg[0][0], ' ', self.y_avg[47][47]
	


# DEBUG Only: Function for plotting graph
def plot_graph(which, ix, rng1=0, rng2=0, nbins=100, norm=0):
	if which == 'y_hist':
	    figure(1)
	    vectorplot (topo.sim['V1'].output_fn.y_hist[ix][rng1:rng2])
	elif which == 'y_rate':
	    figure(2)
	    vectorplot (topo.sim['V1'].output_fn.y_rate[ix][rng1:rng2])
	elif which == 'x_rate':
	    figure(3)
	    vectorplot (topo.sim['V1'].output_fn.x_rate[ix][rng1:rng2])
	elif which == 'y_bins':
	    figure(4)
	    print hist(topo.sim['V1'].output_fn.y_rate[ix][rng1:rng2],nbins,norm)
	    #vectorplot (topo.sim['V1'].output_fn.y_bins[time][ix][rng1:rng2])
	elif which == 'x_bins':
	    figure(5)
	    print hist(topo.sim['V1'].output_fn.x_rate[ix][rng1:rng2],nbins,norm)
	    #vectorplot (topo.sim['V1'].output_fn.x_bins[time][ix][rng1:rng2])
	elif which == 'a':
	    figure(6)
	    vectorplot (topo.sim['V1'].output_fn.ab_hist[0][ix][rng1:rng2])
	elif which == 'b':
	    figure(7)
	    vectorplot (topo.sim['V1'].output_fn.ab_hist[1][ix][rng1:rng2])
	    


###########################################
# build simulation

topo.sim['Retina']=GeneratorSheet(nominal_density=24.0,
                                  input_generator=input_pattern,
                                  period=1.0, phase=0.05,
                                  nominal_bounds=BoundingBox(radius=0.5+0.275))


topo.sim['V1'] = LISSOM(nominal_density=locals().get('default_density',48.0),
                        nominal_bounds=BoundingBox(radius=0.5),output_fn=HomeostaticMaxEnt())

#topo.sim['V1'].output_fn.lower_bound=0.1
#topo.sim['V1'].output_fn.upper_bound=0.65

topo.sim.connect('Retina','V1',delay=FixedPoint("0.10"),
                  connection_type=CFProjection,strength=1.0,name='Afferent',
                  nominal_bounds_template=BoundingBox(radius=0.275),learning_rate=0.9590)

topo.sim.connect('V1','V1',delay=FixedPoint("0.05"),name='LateralExcitatory',
                  connection_type=CFProjection,strength=0.9,
                  nominal_bounds_template=BoundingBox(radius=0.10),learning_rate=3.2018) 
            
topo.sim.connect('V1','V1',delay=FixedPoint("0.05"),name='LateralInhibitory',
                  connection_type=CFProjection,strength=-0.9,
                  nominal_bounds_template=BoundingBox(radius=0.23),learning_rate=1.9626)  



### Actions scheduled to occur as the simulation proceeds.#


topo.sim.startup_commands.append("from topo.base.boundingregion import BoundingBox")

### Lateral excitatory bounds changes
topo.sim.schedule_command(200,'topo.sim["V1"].projections()["LateralExcitatory"].change_bounds(BoundingBox(radius=0.06))')
topo.sim.schedule_command(500,'topo.sim["V1"].projections()["LateralExcitatory"].change_bounds(BoundingBox(radius=0.042))')
topo.sim.schedule_command(1000,'topo.sim["V1"].projections()["LateralExcitatory"].change_bounds(BoundingBox(radius=0.0336))')
topo.sim.schedule_command(2000,'topo.sim["V1"].projections()["LateralExcitatory"].change_bounds(BoundingBox(radius=0.02688))')
topo.sim.schedule_command(3000,'topo.sim["V1"].projections()["LateralExcitatory"].change_bounds(BoundingBox(radius=0.02150))')
topo.sim.schedule_command(4000,'topo.sim["V1"].projections()["LateralExcitatory"].change_bounds(BoundingBox(radius=0.01290))')
topo.sim.schedule_command(5000,'topo.sim["V1"].projections()["LateralExcitatory"].change_bounds(BoundingBox(radius=0.00774))')
topo.sim.schedule_command(6500,'topo.sim["V1"].projections()["LateralExcitatory"].change_bounds(BoundingBox(radius=0.00464))')
topo.sim.schedule_command(8000,'topo.sim["V1"].projections()["LateralExcitatory"].change_bounds(BoundingBox(radius=0.00279))')
topo.sim.schedule_command(20000,'topo.sim["V1"].projections()["LateralExcitatory"].change_bounds(BoundingBox(radius=0.00167))')


### Lateral excitatory learning rate changes
topo.sim.schedule_command(200,'topo.sim["V1"].projections()["LateralExcitatory"].learning_rate=1.2213')
topo.sim.schedule_command(500,'topo.sim["V1"].projections()["LateralExcitatory"].learning_rate=0.3466')


### Afferent learning rate changes
topo.sim.schedule_command(  500,'topo.sim["V1"].projections()["Afferent"].learning_rate=0.6850;topo.sim["V1"].projections()["Afferent"].learning_rate=0.6850')
topo.sim.schedule_command( 2000,'topo.sim["V1"].projections()["Afferent"].learning_rate=0.5480;topo.sim["V1"].projections()["Afferent"].learning_rate=0.5480')
topo.sim.schedule_command( 4000,'topo.sim["V1"].projections()["Afferent"].learning_rate=0.4110;topo.sim["V1"].projections()["Afferent"].learning_rate=0.4110')
topo.sim.schedule_command(20000,'topo.sim["V1"].projections()["Afferent"].learning_rate=0.2055;topo.sim["V1"].projections()["Afferent"].learning_rate=0.2055')



### Schedule LISSOM output function threshold changes
#
'''
topo.sim.schedule_command(  200, 'topo.sim["V1"].output_fn.lower_bound=0.11; topo.sim["V1"].output_fn.upper_bound=0.66')
topo.sim.schedule_command(  500, 'topo.sim["V1"].output_fn.lower_bound=0.12; topo.sim["V1"].output_fn.upper_bound=0.67')
topo.sim.schedule_command( 1000, 'topo.sim["V1"].output_fn.lower_bound=0.15; topo.sim["V1"].output_fn.upper_bound=0.68')
topo.sim.schedule_command( 2000, 'topo.sim["V1"].output_fn.lower_bound=0.18; topo.sim["V1"].output_fn.upper_bound=0.70')
topo.sim.schedule_command( 3000, 'topo.sim["V1"].output_fn.lower_bound=0.20; topo.sim["V1"].output_fn.upper_bound=0.73')
topo.sim.schedule_command( 4000, 'topo.sim["V1"].output_fn.lower_bound=0.20; topo.sim["V1"].output_fn.upper_bound=0.76')
topo.sim.schedule_command( 5000, 'topo.sim["V1"].output_fn.lower_bound=0.21; topo.sim["V1"].output_fn.upper_bound=0.79')
topo.sim.schedule_command( 6500, 'topo.sim["V1"].output_fn.lower_bound=0.22; topo.sim["V1"].output_fn.upper_bound=0.82')
topo.sim.schedule_command( 8000, 'topo.sim["V1"].output_fn.lower_bound=0.23; topo.sim["V1"].output_fn.upper_bound=0.85')
topo.sim.schedule_command(20000, 'topo.sim["V1"].output_fn.lower_bound=0.24; topo.sim["V1"].output_fn.upper_bound=0.88')
'''

topo.sim.run(0)


# Uncomment to allow profiling
## import hotshot,hotshot.stats
## prof = hotshot.Profile("abc")
## prof.runctx('topo.sim.run(199)',globals(),locals())
## prof.close()
## p = hotshot.stats.load("abc")
## p.strip_dirs().sort_stats('cumulative','time').print_stats()
