""" 
Simulation for development of stable orientation maps based on
lissom_oo_or_noshrinking.ty (see Chapman et. al, J. Neurosci
16(20):6443, 1996).  The development is also robust to changes in the
input (e.g., number of Gaussians in the retinal input or brightness of
each Gaussian).

Afferent projection activities for individual units are scaled based
on maintaining a target average afferent activity. The learning rates
of afferent projections are also scaled in order to maintain the
overall rate of learning despite changes in LGN activity.

The sigmoidal output function for V1 is regulated using the Triesch
rule (Jochen Triesch, ICANN 2005, LNCS 3696 pp.65-70), which maintains
stable activity in V1 given a target average activity. 

The afferent target and the target V1 activity are adapted based on
the frequency of occurance of activation in V1 by setting the
frequency parameter. A simulation using 2 gaussian patterns per
iteration as the retinal input has a frequency parameter of 2
therefore frequency values should be chosen relative to this, i.e. a
simulation using 4 gaussian patterns per iteration has an optimum
frequency parameter of ~ 3.56, natural images have a frequency ~ 5.0
and noisy disks a frequency ~ 7.5. 

The balance parameter which adjusts the balance between afferent input
and V1 activity can also be adjusted to improve map organization.

This script is set up as a tracking version for plotting sheet and
projection attributes throughout development and for reproducing the
published results which include examples where scaling is turned off.

To turn tracking off set tracking=False

Not yet fully tested.
$Id$
"""
__version__='$Revision: 8197 $'

import fixedpoint
import numpy

from math import pi, sqrt
from fixedpoint import FixedPoint

import topo.patterns.basic
import topo.patterns.random

from topo.patterns.basic import Gaussian
from topo.sheets.lissom import JointScaling, LISSOM
from topo.sheets.generatorsheet import GeneratorSheet
from topo.projections.basic import CFProjection, SharedWeightCFProjection
from topo.responsefns.optimized import CFPRF_DotProduct_opt
from topo.base.cf import CFSheet, CFPLF_PluginScaled, CFPOF_Plugin
from topo.base.boundingregion import BoundingBox
from topo.learningfns.optimized import CFPLF_Hebbian_opt
from topo.outputfns.optimized import CFPOF_DivisiveNormalizeL1_opt
from topo.outputfns.basic import PiecewiseLinear, DivisiveNormalizeL1, PipelineOF, IdentityOF, ActivityAveragingOF, AttributeTrackingOF 
from topo.outputfns.basic import Sigmoid, HalfRectify, HomeostaticMaxEnt
from topo.misc.numbergenerators import UniformRandom, BoundedNumber, ExponentialDecay
from topo.patterns.image import Image

#########################################################################
##Sheet types only used in this script for reproducing published figures##

from topo.base.parameterclasses import Number
class JointScaling_lronly(LISSOM):
    """

    LISSOM sheet extended to allow auto-scaling of Afferent
    projection learning rate.
    
    An exponentially weighted average is used to calculate the average
    joint activity across all jointly-normalized afferent projections.
    This average is then used to calculate a scaling factor for the
    afferent learning rate.

    The target activity for learning rate scaling is set based on
    original values from the lissom_oo_or.ty simulation using two
    Gaussians per iteration as the input dataset.  """

    target_lr = Number(default=0.045, doc="""
        Target average activity for jointly scaled projections.

        Used for calculating a learning rate scaling factor.""")
    
    smoothing = Number(default=0.999, doc="""
        Influence of previous activity, relative to current, for computing the average.""")

    
   
    def __init__(self,**params):
        super(JointScaling_lronly,self).__init__(**params)
        self.lr_x_avg=None
        self.lr_sf=None
                

    def calculate_joint_sf(self, joint_total):
        """
        Calculate current scaling factors based on the target and previous average joint activities.

        Keeps track of the scaled average for debugging. Could be
        overridden by a subclass to calculate the factors differently.
        """
      
        if self.plastic:
            self.lr_sf *=0.0
            self.lr_sf += self.target_lr/self.lr_x_avg
            self.lr_x_avg = (1.0-self.smoothing)*joint_total + self.smoothing*self.lr_x_avg
            


    def do_joint_scaling(self):
        """
        Scale jointly normalized projections together.

        Assumes that the projections to be jointly scaled are those
        that are being jointly normalized.  Calculates the joint total
        of the grouped projections, and uses this to calculate the
        scaling factor.
        """
        joint_total = zeros(self.shape, activity_type)
        
        for key,projlist in self._grouped_in_projections('JointNormalize'):
            if key is not None:
                if key =='Afferent':
                    for proj in projlist:
                        joint_total += proj.activity
                    self.calculate_joint_sf(joint_total)
                    for proj in projlist:
                        if hasattr(proj.learning_fn,'learning_rate_scaling_factor'):
                            proj.learning_fn.update_scaling_factor(self.lr_sf)
                        else:
                            raise ValueError("Projections to be joint scaled must have a learning_fn that supports scaling e.g. CFPLF_PluginScaled")
                        
                else:
                    raise ValueError("Only Afferent scaling currently supported")                  


    def activate(self):
        """
        Compute appropriate scaling factors, apply them, and collect resulting activity.

        Scaling factors are first computed for each set of jointly
        normalized projections, and the resulting activity patterns
        are then scaled.  Then the activity is collected from each
        projection, combined to calculate the activity for this sheet,
        and the result is sent out.
        """
        self.activity *= 0.0

        if self.lr_x_avg is None:
            self.lr_x_avg=self.target_lr*ones(self.shape, activity_type)
        if self.lr_sf is None:
            self.lr_sf=ones(self.shape, activity_type)

        #Afferent projections are only activated once at the beginning of each iteration
        #therefore we only scale the projection activity and learning rate once.
        if self.activation_count == 0: 
            self.do_joint_scaling()   

        for proj in self.in_connections:
            self.activity += proj.activity
        
        if self.apply_output_fn:
            self.output_fn(self.activity)
           
          
        self.send_output(src_port='Activity',data=self.activity)

       
class JointScaling_affonly(LISSOM):
    """
    LISSOM sheet extended to allow joint auto-scaling of Afferent input projections.
    
    An exponentially weighted average is used to calculate the average
    joint activity across all jointly-normalized afferent projections.
    This average is then used to calculate a scaling factor for the
    current afferent activity.

    The target average activity for the afferent projections depends
    on the statistics of the input; if units are activated more often
    (e.g. the number of Gaussian patterns on the retina during each
    iteration is increased) the target average activity should be
    larger in order to maintain a constant average response to similar
    inputs in V1. 
    """
    target = Number(default=0.045, doc="""
        Target average activity for jointly scaled projections.""")

    smoothing = Number(default=0.999, doc="""
        Influence of previous activity, relative to current, for computing the average.""")

         
    def __init__(self,**params):
        super(JointScaling_affonly,self).__init__(**params)
        self.x_avg=None
        self.sf=None
        self.scaled_x_avg=None


    def calculate_joint_sf(self, joint_total):
        """
        Calculate current scaling factors based on the target and previous average joint activities.

        Keeps track of the scaled average for debugging. Could be
        overridden by a subclass to calculate the factors differently.
        """
      
        if self.plastic:
            self.sf *=0.0
            self.sf += self.target/self.x_avg
            self.x_avg = (1.0-self.smoothing)*joint_total + self.smoothing*self.x_avg
            self.scaled_x_avg = (1.0-self.smoothing)*joint_total*self.sf + self.smoothing*self.scaled_x_avg


    def do_joint_scaling(self):
        """
        Scale jointly normalized projections together.

        Assumes that the projections to be jointly scaled are those
        that are being jointly normalized.  Calculates the joint total
        of the grouped projections, and uses this to calculate the
        scaling factor.
        """
        joint_total = zeros(self.shape, activity_type)
        
        for key,projlist in self._grouped_in_projections('JointNormalize'):
            if key is not None:
                if key =='Afferent':
                    for proj in projlist:
                        joint_total += proj.activity
                    self.calculate_joint_sf(joint_total)
                    for proj in projlist:
                        proj.activity *= self.sf
                                         
                else:
                    raise ValueError("Only Afferent scaling currently supported")                  


    def activate(self):
        """
        Compute appropriate scaling factors, apply them, and collect resulting activity.

        Scaling factors are first computed for each set of jointly
        normalized projections, and the resulting activity patterns
        are then scaled.  Then the activity is collected from each
        projection, combined to calculate the activity for this sheet,
        and the result is sent out.
        """
        self.activity *= 0.0

        if self.x_avg is None:
            self.x_avg=self.target*ones(self.shape, activity_type)
        if self.scaled_x_avg is None:
            self.scaled_x_avg=self.target*ones(self.shape, activity_type) 
        if self.sf is None:
            self.sf=ones(self.shape, activity_type)

        #Afferent projections are only activated once at the beginning of each iteration
        #therefore we only scale the projection activity and learning rate once.
        if self.activation_count == 0: 
            self.do_joint_scaling()   

        for proj in self.in_connections:
            self.activity += proj.activity
        
        if self.apply_output_fn:
            self.output_fn(self.activity)
           
          
        self.send_output(src_port='Activity',data=self.activity)




###############################################################
####Different input types which can be used for development###
dataset=locals().get('dataset',"Gaussian") #set the input type by choosing the dataset parameter 

if dataset=="Gaussian":
    input_type=Gaussian
    num_inputs=locals().get('num_inputs',2) #in the case where dataset=Gaussian, must also set the number of Gaussians per iteration, default is 2
    inputs=[input_type(x=UniformRandom(lbound=-0.75,ubound=0.75,seed=12+i),
                       y=UniformRandom(lbound=-0.75,ubound=0.75,seed=35+i),
                       orientation=UniformRandom(lbound=-pi,ubound=pi,seed=21+i),
                       size=0.088388, aspect_ratio=4.66667, scale= locals().get('scale', 1.0), bounds=BoundingBox(radius=1.125))
            #Set the contrast of the gaussian patterns by setting the scale parameter.
            for i in xrange(num_inputs)]
    
    combined_inputs = topo.patterns.basic.SeparatedComposite(min_separation=0,generators=inputs)
    
elif dataset=="Natural":
    
    input_type=topo.patterns.image.Image
    image_filenames=["images/shouval/combined%02d.png"%(i+1) for i in xrange(25)]
    inputs=[input_type(filename=f,
                       size=10.0,  #size_normalization='original',(size=10.0)
                       x=UniformRandom(lbound=-0.75,ubound=0.75,seed=12),
                       y=UniformRandom(lbound=-0.75,ubound=0.75,seed=36),
                       orientation=UniformRandom(lbound=-pi,ubound=pi,seed=65))
		for f in image_filenames]

    combined_inputs =topo.patterns.basic.Selector(generators=inputs)

elif dataset=="NoisyDisks":
    disk_scale=locals().get('diskscale',0.35)
    #Set the contrast of the disk pattern by setting the disk_scale parameter, map development also depends on the contrast of the disk edges.
    input_type=topo.patterns.basic.Composite
    inputs=[input_type(operator=numpy.add,
                       generators=[topo.patterns.basic.Disk(x=UniformRandom(lbound=-2.125,ubound=2.125,seed=12),
                                                            y=UniformRandom(lbound=-2.125,ubound=2.125,seed=36),
                                                            size=2.0, aspect_ratio=1.0, scale=disk_scale,
                                                            offset=0.5,
                                                            bounds=BoundingBox(radius=1.125), smoothing=0.1),
                                   topo.patterns.random.UniformRandom(offset=locals().get('rand_offset',-0.5), scale=locals().get('rand_scale',1.0))])]
    #Set the scale of the noise by setting the rand_offset and rand_scale parameters, note that the disk/noise signal ratio also depends on the retinal density      
    combined_inputs =topo.patterns.basic.Selector(generators=inputs)

elif dataset=="Disks":
    disk_scale=locals().get('diskscale',0.5)
    input_type=topo.patterns.basic.Disk
    inputs=[input_type(x=UniformRandom(lbound=-2.125,ubound=2.125,seed=12),
                       y=UniformRandom(lbound=-2.125,ubound=2.125,seed=36),
                       size=2.0, aspect_ratio=1.0, scale=disk_scale,
                       offset=0.5,
                       bounds=BoundingBox(radius=1.125), smoothing=0.1)]
            
    combined_inputs =topo.patterns.basic.Selector(generators=inputs)

elif dataset=="NoisyDiskstoNatural":
    #This dataset mimics pre and post eye-opening development - scheduled changes must also be set to ensure the input pattern changes at simulated eye opening
    disk_scale=locals().get('diskscale',0.35)
    disks_input_type=topo.patterns.basic.Composite
    disks_inputs=[disks_input_type(operator=numpy.add,
                       generators=[topo.patterns.basic.Disk(x=UniformRandom(lbound=-2.125,ubound=2.125,seed=12),
                                                            y=UniformRandom(lbound=-2.125,ubound=2.125,seed=36),
                                                            size=2.0, aspect_ratio=1.0, scale=disk_scale,
                                                            offset=0.5,
                                                            bounds=BoundingBox(radius=1.125), smoothing=0.1),
                                   topo.patterns.random.UniformRandom(offset=locals().get('rand_offset',-0.5), scale=locals().get('rand_scale',1.0))])]

    combined_inputs =topo.patterns.basic.Selector(generators=disks_inputs)      
   
    
    natural_input_type=topo.patterns.image.Image
    image_filenames=["images/shouval/combined%02d.png"%(i+1) for i in xrange(25)]
    natural_inputs=[natural_input_type(filename=f,
                       size=10.0,  #size_normalization='original',(size=10.0)
                       x=UniformRandom(lbound=-0.75,ubound=0.75,seed=12),
                       y=UniformRandom(lbound=-0.75,ubound=0.75,seed=36),
                       orientation=UniformRandom(lbound=-pi,ubound=pi,seed=65))
		for f in image_filenames]

    natural_combined_inputs =topo.patterns.basic.Selector(generators=natural_inputs)

###############################################################################

#Sheet coordinates of units to track for debugging
units=locals().get('units',[(0.0, 0.0), (0.25,0.25), (0.49,0.49)])

#Quickly set sheet density, high_density = True in published figures.

high_density=locals().get('hd',False)
if high_density==True:
    default_density = 100
    default_retinal_density = 50 
else:
    default_density = locals().get('default_density',48)
    default_retinal_density = locals().get('default_retinal_density',default_density/2)
  
#Smoothing value for exponential averaging
smoothing=locals().get('smoothing',0.999)
V1_smoothing=locals().get('V1_smoothing',0.999) # Allows different smoothing for averaging  V1 activity and averaging afferent activity.

#Output functions: Sheets
#LGN
LGN_on_output_fn=HalfRectify()
LGN_off_output_fn=HalfRectify()

#Set targets based on frequency of occurance of V1 activation
frequency=locals().get('frequency',2)

#Target average afferent activity and target average V1 activity set based on
#frequency and balance between afferent and lateral activity
mu=locals().get('mu',0.0045*frequency)
balance = locals().get('balance',4.0)
afferent_target = locals().get('afferent_target',mu*balance)

#V1
tracking=locals().get('tracking', True) # Turn tracking on or off
triesch=locals().get('triesch', True) # Turn homeostatic adjustment of V1 output function on or off
scaling=locals().get('scaling',True) # Turn afferent scaling on or off
lr_scaling=locals().get('lr_scaling',True) # Turn scaling of afferent learning rate on or off


if tracking==True:
    if triesch==True:
        if scaling==True:
            if lr_scaling==True:
                Attrib_Tracker=AttributeTrackingOF(object="topo.sim['V1']", attrib_names=['x_avg', 'sf', 'lr_sf', 'scaled_x_avg'], units=units)
                HE=HomeostaticMaxEnt(smoothing=V1_smoothing,
                                              eta=locals().get('eta',0.016), mu=mu, step=9)
                V1_Tracker=AttributeTrackingOF(object=HE, coordframe="topo.sim['V1']",attrib_names=['a', 'b','y_avg'], units=units, step=9)
                V1_OF=PipelineOF(output_fns=[Attrib_Tracker, HE, V1_Tracker])
            else:
                Attrib_Tracker=AttributeTrackingOF(object="topo.sim['V1']", attrib_names=['x_avg', 'sf', 'scaled_x_avg'], units=units)
                HE=HomeostaticMaxEnt(smoothing=V1_smoothing,
                                              eta=locals().get('eta',0.016), mu=mu, step=9)
                V1_Tracker=AttributeTrackingOF(object=HE, coordframe="topo.sim['V1']",attrib_names=['a', 'b','y_avg'], units=units, step=9)
                V1_OF=PipelineOF(output_fns=[Attrib_Tracker, HE, V1_Tracker]) 
        else:
            if lr_scaling==True:
                Attrib_Tracker=AttributeTrackingOF(object="topo.sim['V1']", attrib_names=['lr_sf'], units=units)
                HE=HomeostaticMaxEnt(smoothing=V1_smoothing,  
                                              eta=locals().get('eta',0.016), mu=mu, step=9)
                V1_Tracker=AttributeTrackingOF(object=HE, coordframe="topo.sim['V1']",attrib_names=['a', 'b','y_avg'], units=units, step=9)
                V1_OF=PipelineOF(output_fns=[Attrib_Tracker, HE, V1_Tracker])
            else:
                HE=HomeostaticMaxEnt(smoothing=V1_smoothing, 
                                              eta=locals().get('eta',0.016), mu=mu, step=9)
                V1_Tracker=AttributeTrackingOF(object=HE,coordframe="topo.sim['V1']", attrib_names=['a', 'b','y_avg'], units=units, step=9)
                V1_OF=PipelineOF(output_fns=[HE, V1_Tracker])
    else:
        if scaling==True:
            Attrib_Tracker=AttributeTrackingOF(object="topo.sim['V1']", attrib_names=['x_avg', 'sf','lr_sf', 'scaled_x_avg'], units=units)
            HE=Sigmoid(r=locals().get('a_init',12),k=locals().get('b_init',-5.5))
            AV=ActivityAveragingOF(smoothing=smoothing,step=1)
            V1_Tracker=AttributeTrackingOF(object=AV,coordframe="topo.sim['V1']", attrib_names=['x_avg'], units=units, step=1)
            V1_OF=PipelineOF(output_fns=[Attrib_Tracker, HE, AV, V1_Tracker])
        else:
            HE=Sigmoid(r=locals().get('a_init',12),k=locals().get('b_init',-5.5))
            AV=ActivityAveragingOF(smoothing=smoothing,step=1)
            V1_Tracker=AttributeTrackingOF(object=AV,coordframe="topo.sim['V1']", attrib_names=['x_avg'], units=units, step=1)
            V1_OF=PipelineOF(output_fns=[HE, AV, V1_Tracker])
else:
    if triesch==True:
        V1_OF=HE=HomeostaticMaxEnt(smoothing=V1_smoothing,
                                            eta=locals().get('eta',0.016), mu=mu, step=9)
        
    else:
        V1_OF=Sigmoid(r=locals().get('a_init',12),k=locals().get('b_init',-5.5))
       
#Output Functions: Projections
#Debugging
#LGNOnAfferent
if tracking==True:
    LGNOn_Avg=ActivityAveragingOF(smoothing=smoothing,step=1)
    LGNOn_Tracker=AttributeTrackingOF(object=LGNOn_Avg,coordframe="topo.sim['V1']", attrib_names=['x_avg'], units=units, step=1)
    LGNOn_OF = PipelineOF(output_fns=[LGNOn_Avg, LGNOn_Tracker])

    #LGNOffAfferent
    LGNOff_Avg=ActivityAveragingOF(smoothing=smoothing,step=1)
    LGNOff_Tracker=AttributeTrackingOF(object=LGNOff_Avg,coordframe="topo.sim['V1']", attrib_names=['x_avg'], units=units, step=1)
    LGNOff_OF = PipelineOF(output_fns=[LGNOff_Avg, LGNOff_Tracker])

    #LateralExcitatory
    LatEx_Avg=ActivityAveragingOF(initial_average=0.0,smoothing=smoothing,step=1)
    LatEx_Tracker=AttributeTrackingOF(object=LatEx_Avg,coordframe="topo.sim['V1']", attrib_names=['x_avg'], units=units, step=1)
    LatEx_OF = PipelineOF(output_fns=[LatEx_Avg, LatEx_Tracker])
    
    #LateralInhibitory
    LatIn_Avg=ActivityAveragingOF(initial_average=0.0,smoothing=smoothing,step=1)
    LatIn_Tracker = AttributeTrackingOF(object=LatIn_Avg,coordframe="topo.sim['V1']", attrib_names=['x_avg'], units=units, step=1)
    LatIn_OF = PipelineOF(output_fns=[LatIn_Avg, LatIn_Tracker])

# Specify weight initialization, response function, and learning function
numpy.random.seed((500,500))

CFProjection.cf_shape = topo.patterns.basic.Disk(smoothing=0.0)
CFProjection.weights_generator = topo.patterns.basic.Constant()
CFProjection.response_fn=CFPRF_DotProduct_opt()
CFProjection.learning_fn=CFPLF_Hebbian_opt()
CFProjection.weights_output_fn=CFPOF_DivisiveNormalizeL1_opt()
SharedWeightCFProjection.response_fn=CFPRF_DotProduct_opt()


# DoG weights for the LGN

centerg   = Gaussian(size=0.07385,aspect_ratio=1.0,output_fn=DivisiveNormalizeL1())
surroundg = Gaussian(size=0.29540,aspect_ratio=1.0,output_fn=DivisiveNormalizeL1())
    
on_weights = topo.patterns.basic.Composite(
    generators=[centerg,surroundg],operator=numpy.subtract)

off_weights = topo.patterns.basic.Composite(
    generators=[surroundg,centerg],operator=numpy.subtract)

#Function for generating Gaussian random initial weights
def gauss_rand(size):
    return topo.patterns.basic.Composite(operator=numpy.multiply, 
                                         generators=[Gaussian(aspect_ratio=1.0, size=size),
                                                     topo.patterns.random.UniformRandom()])

#Whether or not to use divisive weights normalization
norm=locals().get('norm',True)

if norm==False:
    pi=topo.base.cf.CFPOF_Plugin(single_cf_fn=topo.outputfns.basic.IdentityOF())
else:
    pi = None


###########################################
# build simulation

topo.sim['Retina']=GeneratorSheet(nominal_density=default_retinal_density,
                                  input_generator=combined_inputs,
                                  period=1.0, phase=0.05,
                                  nominal_bounds=BoundingBox(radius=0.5+0.25+0.375))

topo.sim['LGNOn']=CFSheet(nominal_density=default_retinal_density,
                          nominal_bounds=BoundingBox(radius=0.5+0.25),
                          output_fn=LGN_on_output_fn,
                          measure_maps=False)

topo.sim['LGNOff']=CFSheet(nominal_density=default_retinal_density,
                           nominal_bounds=BoundingBox(radius=0.5+0.25),
                           output_fn=LGN_off_output_fn,
                           measure_maps=False)


if scaling==False:
    if lr_scaling==True:
        #option for only scaling learning rate but not afferent projection
        topo.sim['V1'] = JointScaling_lronly(nominal_density=default_density,
                                             nominal_bounds=BoundingBox(radius=0.5),tsettle=9,
                                             plastic=True,output_fn=V1_OF,
                                             post_initialization_weights_output_fn=pi,
                                             smoothing=smoothing,target_lr=locals().get('target_lr',0.045))
    else:
        #option for no scaling 
        topo.sim['V1'] = LISSOM(nominal_density=default_density,tsettle=9,
                                nominal_bounds=BoundingBox(radius=0.5),
                                output_fn=V1_OF)
        

else:
    if lr_scaling==True:
        #option for scaling both afferent projection and afferent learning rate 
        topo.sim['V1'] = JointScaling(nominal_density=default_density,
                                      nominal_bounds=BoundingBox(radius=0.5),tsettle=9,
                                      plastic=True,output_fn=V1_OF,
                                      post_initialization_weights_output_fn=pi,
                                      target=afferent_target, smoothing=smoothing,
                                      target_lr=locals().get('target_lr',0.045))
    else:
        #option for scaling only afferent projection but not afferent learning rate 
        topo.sim['V1'] = JointScaling_affonly(nominal_density=default_density,
                                              nominal_bounds=BoundingBox(radius=0.5),tsettle=9,
                                              plastic=True,output_fn=V1_OF,
                                              post_initialization_weights_output_fn=pi,
                                              target=afferent_target, smoothing=smoothing)


topo.sim.connect('Retina','LGNOn',delay=FixedPoint("0.05"),
                 connection_type=SharedWeightCFProjection,strength=locals().get('ret_strength',2.33),
                 nominal_bounds_template=BoundingBox(radius=0.375),name='Afferent',
                 weights_generator=on_weights)

topo.sim.connect('Retina','LGNOff',delay = FixedPoint("0.05"),
                 connection_type=SharedWeightCFProjection,strength=locals().get('ret_strength',2.33),
                 nominal_bounds_template=BoundingBox(radius=0.375),name='Afferent',
                 weights_generator=off_weights)

topo.sim.connect('LGNOn','V1',delay=FixedPoint("0.05"), dest_port=('Activity','JointNormalize', 'Afferent'),
                 connection_type=CFProjection,
                 learning_fn=CFPLF_PluginScaled(),
                 strength=1.0,name='LGNOnAfferent',
                 weights_generator=gauss_rand(size=2*0.27083),
                 nominal_bounds_template=BoundingBox(radius=0.27083),
		 learning_rate=locals().get('aff_lr',0.137))
               		 
topo.sim.connect('LGNOff','V1',delay=FixedPoint("0.05"), dest_port=('Activity','JointNormalize', 'Afferent'),
		 connection_type=CFProjection,
                 learning_fn=CFPLF_PluginScaled(),
                 strength=1.0,name='LGNOffAfferent',
                 weights_generator=gauss_rand(size=2*0.27083),
                 nominal_bounds_template=BoundingBox(radius=0.27083),
		 learning_rate=locals().get('aff_lr',0.137))

topo.sim.connect('V1','V1',delay=FixedPoint("0.05"),name='LateralExcitatory',
                 connection_type=CFProjection,
                 strength=1.0*locals().get('exc_strength',1.0),
                 weights_generator=topo.patterns.basic.Gaussian(aspect_ratio=1.0, size=0.04),
                 nominal_bounds_template=BoundingBox(radius=0.03),learning_rate=0.0) 
        
topo.sim.connect('V1','V1',delay=FixedPoint("0.05"),name='LateralInhibitory',
                 connection_type=CFProjection,
                 strength=-1.0*locals().get('inh_strength',1.0),
                 #inh_strength should be increased for more distributed datasets i.e. when the frequency parameter is higher
                 weights_generator=gauss_rand(size=2*0.22917),
                 nominal_bounds_template=BoundingBox(radius=0.22917),learning_rate=locals().get('lat_lr',1.80873))


#Output functions for tracking
if tracking==True:
    topo.sim["V1"].projections()["LGNOnAfferent"].output_fn=LGNOn_OF
    topo.sim["V1"].projections()["LGNOffAfferent"].output_fn=LGNOff_OF
    topo.sim["V1"].projections()["LateralExcitatory"].output_fn=LatEx_OF
    topo.sim["V1"].projections()["LateralInhibitory"].output_fn=LatIn_OF


# default locations for model editor
topo.sim.grid_layout([[None,    'V1',     None],
                      ['LGNOn', None,     'LGNOff'],
                      [None,    'Retina', None]], xstart=150)

### Input pattern changes
changetime = locals().get('changetime',6000)# Time at which patterns or strengths are set to change

changetargets = locals().get('changetargets',True) #If false, targets for afferent scaling and output function adjustment are not changed.
if dataset=="NoisyDiskstoNatural":
    if changetargets==True:
        new_frequency = locals().get('new_frequency',5)
        new_balance = locals().get('new_balance',4)
        new_mu=0.0045*new_frequency
        new_afferent_target = new_mu*new_balance
        topo.sim.schedule_command(changetime,'topo.sim["Retina"].set_input_generator(natural_combined_inputs,push_existing=False)')
        topo.sim.schedule_command(changetime,'topo.sim["V1"].target=new_afferent_target')
        if tracking==True:
            topo.sim.schedule_command(changetime,'topo.sim["V1"].output_fn.output_fns[1].mu=new_mu')
        else:
            topo.sim.schedule_command(changetime,'topo.sim["V1"].output_fn.mu=new_mu')
    else:
        topo.sim.schedule_command(changetime,'topo.sim["Retina"].set_input_generator(natural_combined_inputs,push_existing=False)')


#can set strength of retina to lgn projections to change during development  
changestrength = locals().get('changestrength',False)
if changestrength==True:
    new_strength = locals().get('new_strength',2.0)
    topo.sim.schedule_command(changetime,'topo.sim["LGNOn"].projections()["Afferent"].strength=new_strength')
    topo.sim.schedule_command(changetime,'topo.sim["LGNOff"].projections()["Afferent"].strength=new_strength')


topo.commands.analysis.plotgroups["Orientation Preference"].update_command="measure_or_pref(pattern_presenter=PatternPresenter(pattern_generator=SineGrating(),apply_output_fn=True,duration=1.0))"
    
#############################################################################
#Functions for analysis#####################################################



class StoreMedSelectivity(ParameterizedObject):
    """
    Plots and/or stores the average (median) selectivity across the map.
    """
       
    def __init__(self):
        self.select=[]

    def __call__(self, sheet, filename, init_time, final_time, **params):
        
        selectivity=topo.sim[sheet].sheet_views['OrientationSelectivity'].view()[0]
        avg=median(selectivity.flat)
        self.select.append((topo.sim.time(),avg))

        pylab.figure(figsize=(6,4))
        pylab.grid(True)
        pylab.ylabel("Average Selectivity")
        pylab.xlabel('Iteration Number')
       
        plot_y=[y for (x,y) in self.select]
        plot_x=[x for (x,y) in self.select]
        pylab.plot(plot_x, plot_y)
        plot_data= zip(plot_x, plot_y)
        #print plot_data
        (ymin,ymax)=params.get('ybounds',(None,None))
        pylab.axis(xmin=init_time,xmax=final_time, ymin=ymin, ymax=ymax)
        # The size * the dpi gives the final image size
        #   a 4"x4" image * 80 dpi ==> 320x320 pixel image
        pylab.savefig(normalize_path("MedianSelectivity"+sheet+".png"), dpi=100)
        #save(normalize_path("MedianSelectivity"+sheet),plot_data,fmt='%.6f', delimiter=',') #uncomment if you also want to save the raw data


class StoreStability(ParameterizedObject):
    """

    Plots and/or stores the orientation similarity index across the map
    during development (as defined in Chapman et. al, J. Neurosci
    16(20):6443, 1996).  Each previous stored timepoint is compared
    with the current timepoint.

    """

    
    def __init__(self):
        self.pref_dict={}

    def __call__(self, sheet, filename, init_time, final_time, **params):

        self.pref_dict[topo.sim.time()]=topo.sim[sheet].sheet_views['OrientationPreference'].view()[0]
       

        ##Comparisons
        times = sorted(self.pref_dict.keys())
        values=[]

        for time in times:
            difference = abs(self.pref_dict[topo.sim.time()]-self.pref_dict[time])
            rows,cols = difference.shape
            for r in range(rows):
                for c in range(cols):
                    if difference[r,c] >= 0.5:
                        difference[r,c] = 1-difference[r,c]

            difference *= 2.0
            avg_diff=sum(difference.flat)/len(difference.flat)
            value = 1-avg_diff
            values.append(value)
            
            
        plot_data= zip(times,values)
        #save(normalize_path(filename+str(topo.sim.time())),plot_data,fmt='%.6f', delimiter=',') # uncomment if you want to save the raw data

        pylab.figure(figsize=(6,4))
        pylab.grid(True)
        pylab.ylabel("Average Stability")
        pylab.xlabel('Iteration Number')
       
        pylab.plot(times,values)
        (ymin,ymax)=params.get('ybounds',(None,None))
        pylab.axis(xmin=init_time,xmax=final_time, ymin=ymin, ymax=ymax)
        # The size * the dpi gives the final image size
        #   a 4"x4" image * 80 dpi ==> 320x320 pixel image
        pylab.savefig(normalize_path("AverageStability"+sheet+str(topo.sim.time())+".png"), dpi=100)


# Used by homeostatic analysis function
default_analysis_plotgroups=["Activity", "Orientation Preference"]
Selectivity=StoreMedSelectivity()
Stability=StoreStability()

def homeostatic_analysis_function():
    """

    Analysis function specific to this simulation which can be used in
    batch mode to plots and/or store tracked attributes during
    development.

    """
    #JLALERT Have not yet included saving and restoring state and changing
    #output function and scaling parameters during map measurement, therefore
    #selectivity plots will not reflect true selectivity.

    import topo
    import copy
    from topo.commands.analysis import save_plotgroup, PatternPresenter, update_activity
    from topo.base.projection import ProjectionSheet
    from topo.sheets.generatorsheet import GeneratorSheet
    from topo.patterns.basic import Gaussian
    from topo.commands.basic import pattern_present, wipe_out_activity
    from topo.base.simulation import EPConnectionEvent


    # Build a list of all sheets worth measuring
    f = lambda x: hasattr(x,'measure_maps') and x.measure_maps
    measured_sheets = filter(f,topo.sim.objects(ProjectionSheet).values())
    input_sheets = topo.sim.objects(GeneratorSheet).values()
    
    # Set potentially reasonable defaults; not necessarily useful
    topo.commands.analysis.coordinate=(0.0,0.0)
    if input_sheets:    topo.commands.analysis.input_sheet_name=input_sheets[0].name
    if measured_sheets: topo.commands.analysis.sheet_name=measured_sheets[0].name

    # Save all plotgroups listed in default_analysis_plotgroups
    for pg in default_analysis_plotgroups:
        save_plotgroup(pg)

    # Plot all projections for all measured_sheets
    for s in measured_sheets:
        for p in s.projections().values():
            save_plotgroup("Projection",projection=p)

    Selectivity("V1", "Selectivity", 0, topo.sim.time())
    Stability("V1", "Stability", 0, topo.sim.time())
    #Uncomment to present and record the response to a single gaussian
    #topo.sim.run(1)
    #pattern_present(inputs={"Retina":Gaussian(aspect_ratio=4.66667, size=0.088388, x=0.0, y=0.0, scale=1.0,orientation=0.0 )},
    #                duration=1.0, plastic=False,apply_output_fn=True,overwrite_previous=False)
    #update_activity()
    #save_plotgroup("Activity")

    if __main__.__dict__['tracking'] == True:
        if __main__.__dict__['triesch'] == True: 
            if __main__.__dict__['scaling'] == True:
                plot_tracked_attributes(topo.sim["V1"].output_fn.output_fns[0], 0, 
                                        topo.sim.time(), filename="Afferent", ylabel="Afferent", raw=True)
                plot_tracked_attributes(topo.sim["V1"].output_fn.output_fns[2], 0, 
                                        topo.sim.time(), filename="V1", ylabel="V1", raw=True)
            else:
                if __main__.__dict__['lr_scaling']==True:
                    plot_tracked_attributes(topo.sim["V1"].output_fn.output_fns[0], 0, 
                                            topo.sim.time(), filename="Afferent", ylabel="Afferent", raw=False)
                    plot_tracked_attributes(topo.sim["V1"].output_fn.output_fns[2], 0, 
                                            topo.sim.time(), filename="V1", ylabel="V1", raw=False)
                    
                else:
                    plot_tracked_attributes(topo.sim["V1"].output_fn.output_fns[1], 0, 
                                            topo.sim.time(), filename="Afferent", ylabel="Afferent", raw=False)
          
        else:
            if __main__.__dict__['scaling'] == True:
                plot_tracked_attributes(topo.sim["V1"].output_fn.output_fns[0], 0, 
                                        topo.sim.time(), filename="Afferent", ylabel="Afferent", raw=True)
                plot_tracked_attributes(topo.sim["V1"].output_fn.output_fns[3], 0, 
                                        topo.sim.time(), filename="V1", ylabel="V1", raw=True)
            else:
                plot_tracked_attributes(topo.sim["V1"].output_fn.output_fns[2], 0, 
                                        topo.sim.time(), filename="V1", ylabel="V1", raw=True)
                
            

        plot_tracked_attributes(topo.sim["V1"].projections()["LateralExcitatory"].output_fn.output_fns[1], 0,
                                topo.sim.time(), filename="LatExBefore", ylabel="LatExBefore")
        plot_tracked_attributes(topo.sim["V1"].projections()["LateralInhibitory"].output_fn.output_fns[1], 0,
                                topo.sim.time(), filename="LatInBefore", ylabel="LatInBefore")
